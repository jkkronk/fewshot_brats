Name:  train_teacher_1 Lr_rate:  0.001 Use Teacher:  True  Riter:  200  Subjs:  5
Using device: cuda:0
['Brats17_2013_7_1_t2_unbiased.nii.gz', 'Brats17_TCIA_409_1_t2_unbiased.nii.gz', 'Brats17_CBICA_ABY_1_t2_unbiased.nii.gz', 'Brats17_TCIA_351_1_t2_unbiased.nii.gz']
validation subject ['Brats17_CBICA_AYI_1_t2_unbiased.nii.gz']
Loading train set for subj
Subject  Brats17_2013_7_1_t2_unbiased.nii.gz  Number of Slices:  131
Traceback (most recent call last):
  File "train_restore_MAP_NN.py", line 147, in <module>
    seg, seg_teacher, optimizer, step_rate, log_freq)
  File "/scratch_net/biwidl214/jonatank/code_home/restor_MAP/restoration.py", line 239, in train_run_map_NN_teacher
    dice_loss_teacher.backward()
  File "/scratch_net/biwidl214/jonatank/anaconda3/envs/JKMT/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/scratch_net/biwidl214/jonatank/anaconda3/envs/JKMT/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [32, 128, 128]], which is output 0 of AddBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
