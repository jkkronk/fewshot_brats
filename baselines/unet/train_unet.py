__author__ = 'jonatank'
import sys
sys.path.insert(0, '/scratch_net/biwidl214/jonatank/code_home/restor_MAP/')

import torch
import torch.utils.data as data
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

from models.unet import UNET, train_unet, valid_unet
import argparse
import yaml
import pickle
import numpy as np
import random

from datasets import brats_dataset_subj

if __name__ == "__main__":
    # Params init
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_name', type=str, default=0)
    parser.add_argument("--config", required=True, help="path to config")
    parser.add_argument("--subjs", type=float, help="Number of subjects for training")
    parser.add_argument("--aug", type=int)

    opt = parser.parse_args()
    model_name = opt.model_name
    subj_nbr = opt.subjs
    aug = bool(opt.aug)

    with open(opt.config) as f:
        config = yaml.safe_load(f)



    lr_rate = float(config['lr_rate'])
    data_path = config['path']
    epochs = config['epochs']
    batch_size = config["batch_size"]
    img_size = config["spatial_size"]
    log_freq = config['log_freq']
    log_dir = config['log_dir']
    log_model = config['model_dir']

    # Cuda
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print('Using device: ' + str(device))

    # Load list of subjects
    #f = open(data_path + 'subj_t2_dict.pkl', 'rb')
    #subj_dict = pickle.load(f)
    #f.close()

    #subj_list_all = list(subj_dict.keys())
    #random.shuffle(subj_list_all)
    #subj_list = subj_list_all[:subj_nbr]
    #if subj_nbr == 1:
    #    subj_list = ['Brats17_CBICA_BFB_1_t2_unbiased.nii.gz']

    #print(subj_list)

    slices = [559, 623, 19401, 19469, 600, 10202, 19506, 10107, 546, 10092, 10097, 10223, 10123, 10193, 10206, 19399, 10180, 10140, 10175, 19463, 19427, 10096, 576, 10203, 10126, 10152, 598, 10222, 19507, 628, 542, 19429, 10155, 672, 19422, 10210, 10166, 10176, 10191, 10172, 582, 638, 568, 631, 663, 19500, 10125, 19426, 536, 10122, 10095, 635, 19424, 538, 19516, 19483, 19437, 10156, 19445, 10213, 19502, 606, 566, 585, 10171, 674, 19470, 10124, 19423, 19433, 19468, 567, 556, 558, 10184, 547, 19442, 578, 10154, 10112, 661, 594, 19504, 10151, 590, 10118, 19459, 10093, 19414, 19418, 617, 19467, 10192, 592, 10129, 19412, 19435, 10127, 10098, 10197, 570, 548, 19503, 10214, 19439, 19480, 19397, 591, 651, 19415, 19478, 10091, 561, 641, 572, 19453, 10103, 19416, 655, 19513, 19472, 664, 19434, 574, 19406, 10215, 10137, 653, 19438, 544, 10130, 622, 643, 539, 19417, 626, 662, 10209, 612, 19395, 10187, 19473, 10207, 10190, 10090, 10221, 10121, 10128, 19496, 670, 10218, 10160, 603, 624, 646, 19490, 613, 637, 10115, 19413, 19521, 640, 10220, 608, 10201, 10185, 669, 10142, 10163, 580, 10196, 550, 629, 10139, 540, 19515, 610, 19407, 19488, 654, 611, 10164, 602, 10087, 10134, 537, 19430, 19451, 10132, 19484, 597, 583, 10104, 650, 552, 10181, 19519, 555, 19494, 19428, 10131, 10204, 19405, 19476, 19475, 19393, 19420, 10116, 10216, 19425, 19499, 10136, 10094, 19465, 675, 596, 10113, 19523, 19409, 10183, 632, 19419, 557, 10182, 607, 615, 19495, 19431, 19486, 584, 19447, 10158, 581, 549, 551, 614, 639, 19482, 10170, 10157, 19501, 656, 19403, 19485, 19456, 10146, 19489, 19511, 19446, 19421, 10111, 19440, 10219, 601, 10167, 10141, 10100, 673, 10162, 19398, 19527, 627, 658, 587, 19400, 660, 19522, 10150, 586, 657, 647, 10194, 619, 10174, 19528, 10195, 19462, 671, 630, 667, 19493, 19509, 19526, 10148, 621, 19461, 564, 10211, 19432, 19452, 10200, 577, 565, 10145, 19514, 10138, 665, 19466, 636, 659, 19520, 579, 19492, 609, 19512, 19411, 10099, 10165, 19450, 605, 10212, 10147, 10101, 571, 19471, 10143, 633, 10110, 19518, 10135, 10108, 10205, 573, 19464, 10173, 10089, 10105, 652, 10189, 19392, 554, 10133, 19449, 666, 616, 19436, 575, 10102, 588, 10153, 19444, 19410, 10217, 10109, 10106, 10120, 10088, 19443, 562, 543, 545, 10179]

        # 4 [20235, 12640, 12672, 12635, 26443, 12617, 20315, 20220, 20227, 20332, 12592, 26463, 26370, 20299, 26399, 20261, 26355, 12662, 26377, 20311, 26344, 20266, 26351, 26413, 26420, 26437, 12647, 20259, 20272, 26403, 26440, 20269, 12650, 26375, 20286, 12538, 26459, 12601, 12638, 20337, 26356, 20233, 20280, 12658, 26361, 26460, 12649, 12534, 26432, 12593, 20314, 12661, 20238, 26425, 20351, 12555, 26362, 26350, 12558, 20221, 12619, 20346, 12544, 20223, 12636, 20338, 12609, 26382, 12624, 20245, 20354, 20241, 26414, 26468, 20349, 20228, 12666, 12570, 20260, 26447, 20285, 12535, 20219, 12569, 26395, 12602, 26406, 26458, 12663, 12651, 12552, 26393, 20253, 20275, 12568, 20226, 20278, 20300, 12669, 12545, 26394, 20255, 12582, 26373, 12596, 12543, 20306, 20313, 26461, 26397, 12597, 12536, 26364, 20240, 12577, 12547, 20339, 20329, 26398, 26470, 12637, 12533, 12613, 26409, 20288, 20323, 26469, 26431, 26352, 20355, 26457, 12610, 20319, 26380, 20357, 20331, 26371, 20296, 20318, 12630, 26445, 12604, 26410, 12625, 26368, 26389, 20251, 26400, 12542, 26396, 26372, 26428, 20320, 26456, 12643, 26418, 20307, 20356, 20304, 12562, 12664, 20341, 26467, 12656, 12654, 20277, 20270, 12605, 26446, 20308, 20333, 12645, 12633, 12671, 12616, 26415, 12572, 20328, 20350, 12571, 26391, 12623, 12557, 20236, 20334, 12648, 26444, 26392, 26427, 20324, 12615, 12561, 12556, 12541, 20343, 12665, 26343, 20268, 26347, 20298, 12600, 26336, 20289, 12627, 20322, 20309, 26434, 20222, 20310, 26335, 26341, 20234, 26435, 20336, 26337, 26448, 12564, 26390, 12646, 26353, 20335, 20257, 12579, 26466, 26424, 26363, 20345, 26449, 12611, 26404, 12606, 26454, 20237, 20344, 26379, 12580, 20283, 20264, 20246, 12621, 12589, 12574, 26401, 12632, 26378, 12590, 20250, 26451, 20290, 12657, 12644, 20294, 26357, 20348, 12578, 20292, 12668, 12642, 20230, 12599, 26354, 26438, 20254, 12575, 20281, 26408, 26345, 26338, 12673, 20282, 20353, 20263, 12581, 26450, 12584, 12631, 20249, 12591, 12551, 12618, 20232, 26374, 20302, 20252, 12620, 20218, 26411, 12634, 20305, 20274, 12540, 20231, 12639, 12563, 20352, 12655, 20287, 12586, 20342, 26365, 12667, 12560, 20303, 20347, 26348, 20291, 20293, 26442, 26423, 26426, 26430, 26436, 12539, 26340, 12622, 26386, 12585, 26360, 20225, 12595, 12598, 26433, 20258, 20248, 26381, 20265, 12566, 20242, 20262, 26416, 20295, 26367, 26359, 26412, 26422, 26339, 12553, 26429, 12550, 26346, 26407, 12537, 12549, 20317, 26369, 12629, 20273, 12559, 12660, 26366, 20330, 12583, 26439, 12567]

    #slices = []
    #for subj in subj_list:  # Iterate every subject
    #    slices.extend(subj_dict[subj])  # Slices for each subject

    # Load data
    subj_dataset = brats_dataset_subj(data_path, 'train', img_size, slices, use_aug=True)
    subj_loader = data.DataLoader(subj_dataset, batch_size=batch_size, shuffle=True, num_workers=3)
    print(' Number of Slices: ', subj_dataset.size)

    # Load list of validation subjects
    f = open(data_path + 'subj_t2_valid_dict.pkl', 'rb')
    val_subj_dict = pickle.load(f)
    f.close()

    #val_subj_list_all = list(val_subj_dict.keys())
    # if subj_nbr == 1:
    #    subj_list = ['Brats17_CBICA_BFB_1_t2_unbiased.nii.gz']

    #print(val_subj_list_all)

    #slices = []
    #for subj in val_subj_list_all:  # Iterate every subject
    #    slices.extend(val_subj_dict[subj])  # Slices for each subject
    slices = [553, 560, 19404, 541, 634, 10117, 648, 10159, 19458, 19474, 10149, 10186, 618, 19402, 10114, 644, 19491, 19391, 19441, 19455, 19477, 642, 19510, 19408, 620, 569, 668, 19517, 593, 10177, 10208, 19498, 19396, 10198, 599, 10178, 10199, 645, 604, 10119, 19481, 19525, 10144, 19508, 595, 649, 10161, 19487, 19457, 19524, 589, 19394, 19448, 19454, 10169, 625, 19497, 19505, 19460, 10168, 19479, 10188, 563]
        # 4 [20224, 20244, 20276, 20301, 20316, 12576, 12588, 12573, 26417, 20325, 20340, 26349, 26405, 12652, 20284, 20297, 26453, 26384, 26421, 26387, 20326, 20279, 20327, 12548, 12587, 12612, 12554, 20321, 12659, 20271, 26419, 26342, 12603, 12614, 26358, 26402, 12626, 26462, 12608, 26455, 12628, 26383, 20229, 12607, 26452, 20312, 20243, 26385, 26376, 12565, 12670, 12641, 12546, 20267, 12653, 20256, 26388, 12594, 20247, 26464, 26465, 26441, 20239]


    #slices =  [19665, 9761, 21680, 13194, 13147, 6917, 19655, 9802, 19653, 11934, 11938, 8311, 13165, 12052, 9776, 21602, 12003, 6855, 21610, 4117, 9710, 6923, 13141, 8249, 21618, 13116, 9803, 4094, 7454, 8214, 11989, 9737, 13167, 19622, 13144, 12093, 4038, 7481, 21687, 6883, 21583, 9703, 19627, 21567, 9754, 8304, 12098, 12109, 13163, 11937, 19540, 7463, 11923, 21679, 6892, 19584, 8276, 9750, 11901, 21688, 12077, 21644, 11910, 8215, 7389, 13154, 4093, 19648, 9789, 13179, 6977, 8256, 19596, 6932, 11890, 6888, 8268, 13138, 12110, 19574, 8206, 11882, 19611, 8252, 7484, 6919, 21658, 8232, 12106, 12105, 6863, 9740, 19663, 7395, 11980, 8193, 21674, 7405, 4124, 7508, 21587, 12113, 7519, 4046, 9759, 4076, 19621, 6889, 6946, 19582, 11961, 9825, 8286, 6851, 6868, 11924, 7493, 9687, 6879, 4140, 21657, 4075, 12087, 9815, 21677, 11988, 4130, 7505, 12123, 13107, 21654, 12041, 4039, 21596, 11986, 12055, 7380, 12046, 9762, 9814, 7379, 12079, 4062, 19573, 11952, 13139, 6902, 4044, 11885, 21634, 12127, 13148, 7421, 9728, 6882, 9726, 21630, 19577, 9692, 6891, 6940, 7402, 21684, 19643, 6890, 19650, 11909, 12090, 13105, 9800, 7400, 12111, 11935, 9686, 7468, 13101, 8315, 19636, 21598, 9769, 7415, 19630, 11981, 9764, 8201, 12036, 8251, 13204, 21571, 4057, 9758, 21574, 12027, 12104, 11998, 19608, 9751, 7382, 19597, 4145]

    # Load validation data
    val_subj_dataset = brats_dataset_subj(data_path, 'train', img_size, slices, use_aug=False)
    val_subj_loader = data.DataLoader(subj_dataset, batch_size=batch_size, shuffle=True, num_workers=3)
    print(' Number of Slices: ', subj_dataset.size)

    # Create unet
    model = UNET(model_name, 1,1,16).to(device)

    # Create optimizer and scheduler
    optimizer = optim.Adam(model.parameters(), lr=lr_rate)

    # Tensorboard Init
    writer_train = SummaryWriter(log_dir + model.name + '_train')
    writer_valid = SummaryWriter(log_dir + model.name + '_valid')

    # Start training
    print('Start training:')
    for epoch in range(epochs):
        print('Epoch:', epoch)

        loss = train_unet(model, subj_loader, device, optimizer)
        loss_valid = valid_unet(model, val_subj_loader, device)

        writer_train.add_scalar('Loss',loss, epoch)
        writer_train.flush()
        writer_valid.add_scalar('Loss',loss_valid, epoch)
        writer_valid.flush()

        if epoch % log_freq == 0 and not epoch == 0:
            data_path = log_model + model_name + str(epoch) + '.pth'
            torch.save(model, data_path)